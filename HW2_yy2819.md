
# Homework 2
author: Yang Yang(yy2819)  
Date: Oct.16 2018


## Problem 1 (HMMs and topics)
Suppose we have to model text data which is streamed from a news feed; each news item is part of a single topic.After a (random) number of words, the new item ends and the next item begins, which in general has a differenttopic.  Over time, topics may repeat.  Suppose we have estimated empirically that:  
- There  are $K$ topics,  and  we  have  estimated  the  probability  vectors $θ_1,...,θ_K$ (where $θ_k$ is  the  parametervector of a multinomial which models text with topick).  
- At any given word, the probability of remaining within the current topic is 0.99.  
- The probability of switching to a different topic is 0.01.  For simplicity, assume all topics are equally probable,so the probability to switch to a specific new topic is $q:=\frac {0.01}{K−1}$for each topic.  

a)  Define a hidden Markov model to model the word sequence $X_1,X_2,....$  Please make sure that you specify:  
- The state space of your model.
- The observed and hidden variables.
- The transition and emission probabilities.




#### Answer:
- States space (Hidden variables): $Z_n \in \{1,...,K\}$  
(K topics).  
- Sample space (Observed variables): $X={1,...,M}$  
(Suppose there are M different words in corpus).  
- Transition matrix: $\mathbf q=    \begin{pmatrix}
    0.99 & q & \cdots & q \\
    q & 0.99 & \cdots & q\\
    \vdots &\vdots  &\ddots  &\vdots  \\
    q & q & \cdots & 0.99\\
    \end{pmatrix}$ ,where $q=\frac{0.01}{K-1}$
- Emission probabilities: $P(x_n|Z_n=k) =P(x_n|θ_k)=\theta_k\ (Multinomial\ Dist.)$

b)  Are the variables Xi and Xi+2 (for any i∈N) stochastically dependent?
#### Answer:
Yes, they are stochastically dependent.  
Only when given any of $Z_i,Z_{i+1},Z_{i+2}$, will they become independent of each other.

## Problem 2 (Gibbs sampling)
For a set $x_1, . . . , x_d$, we write $x_{−i}$ for the set with the *i*th element removed,  
$$x_{−i} = {x_1, . . . , x_{i−1}, x_{i+1}, . . . , x_d}$$

### a) Let p be a Gaussian density on $\Bbb R^d$, with mean $μ$ and covariance $Σ$. Derive the full conditionals $p(x_i|x_{−i})$, for $i \in \{1, . . . , d\}$.  



#### Answer:
Denote $\mathbf x_j:=\mathbf x_{-i}$, then consider partitioning   
$\mathbf x$ into
$\begin{bmatrix}
\mathbf{ x_i}\\
\mathbf{x_{j}} 
\end{bmatrix}$, $\mathbf \mu$ into
$\begin{bmatrix}
\mathbf{\mu_i}\\
\mathbf{\mu_{j}} 
\end{bmatrix}$,  with a similar partition of $\mathbf \Sigma$ into
$\begin{bmatrix}
\Sigma_{ii} & \Sigma_{ij}\\
\Sigma_{ji} & \Sigma_{jj}
\end{bmatrix}$.  
Now Define ${\mathbf z} = {\mathbf x}_i + {\mathbf A} {\mathbf x}_j$, where ${\mathbf A} = -\Sigma_{ij} \Sigma^{-i}_{jj}$, then we have
\begin{align*} {\rm cov}({\mathbf z}, {\mathbf x}_j) &= {\rm cov}( {\mathbf x}_{i}, {\mathbf x}_j ) + 
{\rm cov}({\mathbf A}{\mathbf x}_j, {\mathbf x}_j) \\
&= \Sigma_{ij} + {\mathbf A} {\rm var}({\mathbf x}_j) \\
&= \Sigma_{ij} - \Sigma_{ij} \Sigma^{-i}_{jj} \Sigma_{jj} \\
&= 0
\end{align*}
Therefore $\mathbf z$ and $\mathbf x_j$ are uncorrelated and, since they are jointly normal, they are independent.  $\mathbf z\perp\!\!\!\perp x_j$  
Obviously, $E({\mathbf z}) = {\boldsymbol \mu}_i + {\mathbf A}  {\boldsymbol \mu}_j$, thus the conditional mean is 
$$
\begin{align*}
E({\mathbf x}_i | {\mathbf x}_j) &= E( {\mathbf z} - {\mathbf A} {\mathbf x}_j | {\mathbf x}_j) \\
& = E({\mathbf z}|{\mathbf x}_j) -  E({\mathbf A}{\mathbf x}_j|{\mathbf x}_j) \\
& = E({\mathbf z}) - {\mathbf A}{\mathbf x}_j \\
& = {\boldsymbol \mu}_i + {\mathbf A}  ({\boldsymbol \mu}_j - {\mathbf x}_j) \\
& = {\boldsymbol \mu}_i + \Sigma_{ij} \Sigma^{-i}_{jj} ({\mathbf x}_j- {\boldsymbol \mu}_j)
\end{align*}
$$

and the conditional variance is
$$
\begin{align*}
{\rm var}({\bf x}_i|{\bf x}_j) &= {\rm var}({\bf z} - {\bf A} {\bf x}_j | {\bf x}_j) \\
&= {\rm var}({\bf z}|{\bf x}_j) + {\rm var}({\bf A} {\bf x}_j | {\bf x}_j) - {\bf A}{\rm cov}({\bf z}, -{\bf x}_j) - {\rm cov}({\bf z}, -{\bf x}_j) {\bf A}' \\
&= {\rm var}({\bf z}|{\bf x}_j) \\
&= {\rm var}({\bf z})\\
&= {\rm var}( {\bf x}_i + {\bf A} {\bf x}_j ) \\
&= {\rm var}( {\bf x}_i ) + {\bf A} {\rm var}( {\bf x}_j ) {\bf A}'+ {\bf A} {\rm cov}({\bf x}_i,{\bf x}_j) + {\rm cov}({\bf x}_j,{\bf x}_i) {\bf A}' \\
&= \Sigma_{ii} +\Sigma_{ij} \Sigma^{-i}_{jj} \Sigma_{jj}\Sigma^{-i}_{jj}\Sigma_{ji} - j \Sigma_{ij} \Sigma_{jj}^{-i} \Sigma_{ji} \\
&= \Sigma_{ii} +\Sigma_{ij} \Sigma^{-i}_{jj}\Sigma_{ji}- j \Sigma_{ij} \Sigma_{jj}^{-i} \Sigma_{ji} \\
&= \Sigma_{ii} -\Sigma_{ij} \Sigma^{-i}_{jj}\Sigma_{ji}
\end{align*}
$$

As we know, all conditional distributions of a multivariate normal distribution are normal. Therefore, the full conditionals $p(x_i|x_{−i})=p(x_i|x_{j})\sim \mathcal N({\bf \mu}_i + \Sigma_{ij} \Sigma^{-i}_{jj} ({\bf x}_j- {\bf \mu}_j),\Sigma_{ii} -\Sigma_{ij} \Sigma^{-i}_{jj}\Sigma_{ji})$, for $i \in \{1, . . . , d\}$.

### b) Consider a directed graphical model with graph:

<p align="center">
<img src="https://github.com/jokerkeny/playground/blob/master/fig/graphicalMC.png?raw=true" alt="drawing" width="200" style="text-align:center"/>
  </p>


If $i = 1$, we read $X_{i−1}$ as $X_6$. Suppose each variable $X_i$ takes values in {0, 1}, with conditional
$$P(X_i = 1|X_{i−1}) = σ(θ_iX_{i−1})\qquad  for\ some\ θ_i \in [0, 1],$$ 
where $σ$ is the sigmoid function, given by $σ(y) = \frac 1{1+exp(−y)}$(The original form on HW2 pdf is $σ(y) = \frac 1{1-exp(−y)}$ which I guess is wrong)
. What are the full conditionals $P(X_i = •|X_{−i})$?

#### Ans:
As it's a directed graphical model shown above, we can find $X_i$only depends on $X_{i-1},X_{i+1}$, which means, 
$$P(X_i = •|X_{−i})=P(X_i = •|X_{i+1},X_{i-1})$$

We also know the prior distribution:
$$\begin{aligned}
P(X_i = 1|X_{i−1}=0) &= σ(0)=\frac 12\\
P(X_i = 1|X_{i−1}=1) &= σ(\theta_i)\\
\end{aligned}$$

Then,
$$
\begin{aligned}
P(X_i = 1|X_{i+1}=0,X_{i-1}=0)&=\frac{likelihood\cdot prior}{evidence}\\
&=\frac{P(X_{i+1} = 0|X_{i}=1,X_{i-1}=0)P(X_i = 1|X_{i−1}=0)P(X_{i−1}=0)}
{\sum_{X_{i}\in\{0,1\}} P(X_{i+1} = 0|X_{i},X_{i-1}=0)P(X_i |X_{i−1}=0)P(X_{i−1}=0)}\\
&=\frac{P(X_{i+1} = 0|X_{i}=1)P(X_i = 1|X_{i−1}=0)}
{\sum_{X_{i}\in\{0,1\}} P(X_{i+1} = 0|X_{i})P(X_i |X_{i−1}=0)}(by\ properties\ of\ graphical\ model)\\
&=\frac{(1-σ(\theta_{i+1}))\frac12}{(1-σ(\theta_{i+1}))\frac12+\frac12\cdot\frac12}\\
&=\frac{1-σ(\theta_{i+1})}{1-σ(\theta_{i+1})+\frac12}
\end{aligned}
$$

$$
\begin{aligned}
P(X_i = 1|X_{i+1}=1,X_{i-1}=0)
&=\frac{P(X_{i+1} = 1|X_{i}=1)P(X_i = 1|X_{i−1}=0)}
{\sum_{X_{i}\in\{0,1\}} P(X_{i+1} = 1|X_{i})P(X_i |X_{i−1}=0)}\\
&=\frac{σ(\theta_{i+1})\frac12}{σ(\theta_{i+1})\frac12+\frac12\cdot\frac12}\\
&=\frac{σ(\theta_{i+1})}{σ(\theta_{i+1})+\frac12}
\end{aligned}
$$

$$
\begin{aligned}
P(X_i = 1|X_{i+1}=0,X_{i-1}=1)
&=\frac{P(X_{i+1} = 0|X_{i}=1)P(X_i = 1|X_{i−1}=1)}
{\sum_{X_{i}\in\{0,1\}} P(X_{i+1} = 0|X_{i})P(X_i |X_{i−1}=1)}\\
&=\frac{(1-σ(\theta_{i+1}))σ(\theta_{i})}{(1-σ(\theta_{i+1}))σ(\theta_{i})+\frac12\cdot(1-σ(\theta_{i}))}\\
\end{aligned}
$$

$$
\begin{aligned}
P(X_i = 1|X_{i+1}=1,X_{i-1}=1)
&=\frac{P(X_{i+1} = 1|X_{i}=1)P(X_i = 1|X_{i−1}=1)}
{\sum_{X_{i}\in\{0,1\}} P(X_{i+1} = 1|X_{i})P(X_i |X_{i−1}=1)}\\
&=\frac{σ(\theta_{i+1})σ(\theta_{i})}{σ(\theta_{i+1})σ(\theta_{i})+\frac12\cdot (1-σ(\theta_{i}))}\\
\end{aligned}
$$

Above all is the full conditionals $P(X_i = •|X_{−i})$

## Problem 3 (Implementation)
- Implement a Gibbs sampler for the d-dimensional Gaussian.


```python
import numpy as np
from scipy import linalg
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
```


```python
mu=np.array([3,25,8])
Sigma=np.array([[1,2,3],[4,5,6],[7,8,9]])
print (Sigma)
i=2
n=3
sig_ij=Sigma[i-1,range(i-1).append(range(i,n))]
print(sig_ij)
```

    [[1 2 3]
     [4 5 6]
     [7 8 9]]
    [[4 5 6]]



```python
range(i,n)
```




    [2]




```python
def Ggaussian(mu, Sigma , n):
# Your code here
# suppose that both mu, Sigma input as np.array
# suppose generate n samples
  d=len(mu)
  x=np.empty((n,d))
  x[0,:]=mu # Initialize 
  for k in range(1,n):
    for i in range(d):
      sig_ii=Sigma[i-1,i-1]
      sig_ij=Sigma[i-1,(0:i-1)]
      sig_ji
      sig_jj
      mu_i
      mu_j
      
      mean=
      var=
      
  
  return(np.real(G)) # an n by d matrix of d−dim gaussian vectors
```

- Run the sampler for d = 2 on a Gaussian with mean vector and covariance matrix
$$
μ =\begin{bmatrix}
1\\
1
\end{bmatrix}\qquad
\Sigma=\begin{bmatrix}
1&1\\
1&5
\end{bmatrix},
$$
and visualize the results: Plot the contour lines of Gaussian (for one standard deviation) against, say, 1000 samples of the Gibbs sampler.


- Implement Gibbs sampler for distribution in 2b above, with $θ_1 = . . . = θ_6 = \frac13$. Compare before and after burn-in.


```python
def G2b
```
